# Reward model training configuration
#
# 用于训练 Helpful-RM 与 Harmless-RM，两者共用同一个基础配置，
# 通过命令行参数 --task 控制具体任务。

# =========================
# Base model & tokenizer
# =========================
base_model_path: models/base/Mistral-7B-v0.1
tokenizer_name: null     # 默认使用 base_model_path
max_length: 384  # 在config中保留，但实际训练时会使用TrainConfig中的值

# =========================
# Data paths
# =========================
# 这些路径由 src/scripts/prepare_data.py 生成
train_helpful_path: data/train/helpful_pairs.jsonl
val_helpful_path: data/val/helpful_pairs.jsonl

train_harmless_path: data/train/harmless_pairs.jsonl
val_harmless_path: data/val/harmless_pairs.jsonl

# =========================
# Training hyper-parameters
# =========================
learning_rate: 2e-5      # 建议在 1e-5 ~ 5e-5 之间（使用LoRA时可提高到1e-4）

# 显存优化配置（二选一）：
# 方案A: 全参数微调（需要更多显存）
# batch_size: 1
# max_length: 256
# gradient_accumulation_steps: 16

# 方案B: LoRA微调（推荐，显存占用大幅降低）
batch_size: 4            # LoRA模式下可以使用更大的batch_size
max_length: 512          # LoRA模式下可以使用更长的序列
gradient_accumulation_steps: 2  # LoRA模式下需要较少的梯度累积

num_epochs: 1            # 初次跑通建议 1 epoch
weight_decay: 0.01
max_grad_norm: 1.0

# =========================
# LoRA配置（参数高效微调）
# =========================
use_lora: true           # 启用LoRA以大幅降低显存占用（推荐）
lora:
  r: 16                  # LoRA rank，控制可训练参数数量（r越大参数越多，显存占用越高）
  alpha: 32              # LoRA alpha，通常设为r的2倍
  dropout: 0.1           # LoRA dropout
  # target_modules: null  # 可选：指定要应用LoRA的模块，null时使用默认值（适用于Mistral）


