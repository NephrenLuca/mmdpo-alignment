# Reward model training configuration
#
# 用于训练 Helpful-RM 与 Harmless-RM，两者共用同一个基础配置，
# 通过命令行参数 --task 控制具体任务。

# =========================
# Base model & tokenizer
# =========================
base_model_path: models/base/Mistral-7B-v0.1
tokenizer_name: null     # 默认使用 base_model_path
max_length: 384  # 在config中保留，但实际训练时会使用TrainConfig中的值

# =========================
# Data paths
# =========================
# 这些路径由 src/scripts/prepare_data.py 生成
train_helpful_path: data/train/helpful_pairs.jsonl
val_helpful_path: data/val/helpful_pairs.jsonl

train_harmless_path: data/train/harmless_pairs.jsonl
val_harmless_path: data/val/harmless_pairs.jsonl

# =========================
# Training hyper-parameters
# =========================
learning_rate: 2e-5      # 建议在 1e-5 ~ 5e-5 之间
batch_size: 1            # 每个GPU的batch size，进一步减小以降低显存占用（8x64GB GPU建议1）
max_length: 384          # 序列长度，进一步减小以降低激活显存（8x64GB GPU建议384-512）
gradient_accumulation_steps: 8  # 梯度累积步数，有效batch_size = batch_size * accumulation * num_gpus = 1 * 8 * 8 = 64
num_epochs: 1            # 初次跑通建议 1 epoch
weight_decay: 0.01
max_grad_norm: 1.0


