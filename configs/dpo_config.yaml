# Safe-MM-DPO training configuration
#
# 该配置对应 dev.md 中推荐的超参数，并补充了模型与数据路径。

# =========================
# Safe-MM-DPO 核心超参数
# =========================
λ_init: 1.0
w: 0.5
k: 0.5
β_ori: 0.1

learning_rate: 5e-7      # 使用LoRA时可提高到1e-6或2e-6
batch_size: 4            # LoRA模式下可以使用较小的batch_size（8x64GB GPU建议2-4）
max_length: 512          # 序列最大长度，根据显存调整（8x64GB GPU建议384-512）
kl_coeff: 0.1
λ_lr: 0.01
epochs: 2
gradient_accumulation_steps: 4   # 有效 batch size = batch_size * grad_acc * num_gpus
max_grad_norm: 1.0
warmup_steps: 100

# =========================
# 模型路径
# =========================
# 策略模型与参考模型均初始化为同一个 Mistral-7B
policy_model_path: models/base/Mistral-7B-v0.1
ref_model_path: models/base/Mistral-7B-v0.1

# Two reward models trained via train_rm.py
helpful_rm_path: models/helpful_rm
harmless_rm_path: models/harmless_rm

# =========================
# 数据路径
# =========================
train_helpful_path: data/train/helpful_pairs.jsonl
train_harmless_path: data/train/harmless_pairs.jsonl
val_helpful_path: data/val/helpful_pairs.jsonl
val_harmless_path: data/val/harmless_pairs.jsonl

# =========================
# LoRA配置（参数高效微调）
# =========================
use_lora: true           # 启用LoRA以大幅降低显存占用（强烈推荐）
lora:
  r: 16                  # LoRA rank，控制可训练参数数量
  alpha: 32              # LoRA alpha，通常设为r的2倍
  dropout: 0.1           # LoRA dropout
  # target_modules: null  # 可选：指定要应用LoRA的模块，null时使用默认值（适用于Mistral）

# =========================
# 其他选项（可按需扩展）
# =========================
save_steps: 500           # 可选：若实现按 step 保存
eval_steps: 250           # 可选：若实现按 step 验证


